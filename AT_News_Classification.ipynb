{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import konlpy\n",
    "from konlpy.tag import Twitter\n",
    "import os\n",
    "from os import listdir\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT_DATA_DIR = './newsData'\n",
    "NUM_CATEGORY = 8\n",
    "BATCH_SIZE=10\n",
    "EPOCHS=12\n",
    "SPLIT_RATE=0.8\n",
    "NUM_TEST_PER_DIRECTORY = 160"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unknown token 추가하여 초기화\n",
    "word2idx = {'<unk>':0}\n",
    "idx2word = {0:'<unk>'}\n",
    "vocab=Counter()\n",
    "vocab.update(['<unk>'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Extract_word(directory):\n",
    "    print(\"%s 작업중\"%directory)\n",
    "    count = 1\n",
    "    for filename in listdir(directory):\n",
    "        # train 파일로만 단어장을 만든다.\n",
    "        if count > NUM_TEST_PER_DIRECTORY:\n",
    "            break\n",
    "        path = directory+'/'+filename\n",
    "        \n",
    "        file = open(path,'r',encoding='utf-8')\n",
    "        doc = file.read()\n",
    "        file.close()\n",
    "        \n",
    "        twit = Twitter()\n",
    "        doc = twit.pos(doc)\n",
    "        tag=['Noun','Foreign','Alpha','Number']        \n",
    "        exception=['\\n\\n','\\t','‘','”',',','ㆍ','·','…','“','’','【','】','‧','◀','▶','\\xa0','∼',\"”…\",\"ㆍ‘\",\n",
    "            '것','했',\"있다\",\"등\",\"기자\",\"또\",\"며\",\"그\"]    \n",
    "        for pair in doc:\n",
    "            if pair[1] in tag and pair[0] not in exception:\n",
    "                vocab.update([pair[0]])       \n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'<unk>': 1})\n"
     ]
    }
   ],
   "source": [
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab 만들기, 이미 정의된 vocab이 있는 경우 생략한다.\n",
    "def Make_vocab():\n",
    "    if os.path.exists(\"vocab.txt\"):\n",
    "        print(\"이미 정의된 vocab을 불러옵니다.\")\n",
    "        file=open('vocab.txt','r',encoding='utf-8')\n",
    "        doc=file.read()\n",
    "        doc = doc.split('\\n')\n",
    "        file.close()\n",
    "        return doc\n",
    "    else: \n",
    "        for i in range(NUM_CATEGORY):\n",
    "            Extract_word(TEXT_DATA_DIR+'/'+str(i))\n",
    "        # vocab 저장\n",
    "        data = '\\n'.join(tokens)\n",
    "        file=open('vocab.txt','w',encoding='utf-8')\n",
    "        file.write(data)\n",
    "        file.close()\n",
    "        print('단어 수 :',len(vocab))\n",
    "        print('빈출 상위 50 단어 :')\n",
    "        print(vocab.most_common(50))\n",
    "        return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이미 정의된 vocab을 불러옵니다.\n"
     ]
    }
   ],
   "source": [
    "vocab = Make_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15068\n"
     ]
    }
   ],
   "source": [
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i+1 을 한 이유는 <unk> 이 이미 하나 들어가 있기 때문\n",
    "for i,k in enumerate(vocab):\n",
    "    word2idx[k]=i+1\n",
    "    idx2word[i+1]=k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음에 할 일\n",
    "\n",
    "데이터 \n",
    "- 숫자면 다 숫자 토큰 하나로 변경하기(vocab에 숫자 토큰 추가)\n",
    "- 단어장에 없던 단어가 나오면 <unk> 하나로 합치기\n",
    "- word embedding 적용\n",
    "- 모델 세우기(Attention Is All you need 논문 참고)\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train=[]\n",
    "t_train=[]\n",
    "x_test=[]\n",
    "t_test=[]\n",
    "\n",
    "# 폴더 개수 8개\n",
    "for i in range(NUM_CATEGORY):\n",
    "    path = TEXT_DATA_DIR+'/'+str(i)\n",
    "    print(\"%s folder 작업중...\"%i)\n",
    "    count = 0\n",
    "    for filename in sorted(os.listdir(path)):\n",
    "        filename=path+'/'+filename\n",
    "        temp=Load_doc(filename)\n",
    "        temp=Clean_doc(temp,vocab)\n",
    "        temp=Word_To_Index(temp)\n",
    "        # one hot encoding\n",
    "        tempT=np.zeros(8)\n",
    "        tempT[i]=1\n",
    "        if count<160:\n",
    "            x_train.append(temp)\n",
    "            t_train.append(tempT)\n",
    "        else :\n",
    "            x_test.append(temp)\n",
    "            t_test.append(tempT)\n",
    "        count += 1\n",
    "x_train=np.array(x_train)\n",
    "x_test=np.array(x_test)\n",
    "t_train=np.array(t_train)\n",
    "t_test=np.array(t_test)\n",
    "print(len(x_train))\n",
    "print(len(t_train))\n",
    "print(len(x_test))\n",
    "print(len(t_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building Hierachical Attention Network\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self,attention_size):\n",
    "        super(Attention, self).__init__()\n",
    "        self.attention = new_parameter(attention_size,1)\n",
    "        \n",
    "    def forward(self, x_in):\n",
    "        # after this, we have (batch, dim1) with a diff weight per each cell\n",
    "        attention_score = torch.matmul(x_in, self.attention).squeeze()\n",
    "        attention_score = F.softmax(attention_score).view(x_in.size(0),x_in.size(1),1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
